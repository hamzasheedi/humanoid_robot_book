---
title: Robotics and AI Glossary
---

# Robotics and AI Glossary

This glossary contains key terms and definitions used throughout the AI Robotics Textbook. It is designed to help students understand the specialized vocabulary used in robotics, artificial intelligence, and humanoid robot development.

## A

**Action Space**: The set of possible actions that an agent (such as a robot) can take in a given environment. In robotics, this often corresponds to possible joint movements or motor commands.

**Affordance**: In robotics, the possibility of an action on an object by an agent. For example, a door handle affords turning, or a chair affords sitting.

**Artificial Intelligence (AI)**: The simulation of human intelligence processes by computer systems, especially machine learning and deep learning. In robotics, AI enables robots to perceive, reason, and act autonomously.

## B

**Behavior Tree**: A hierarchical model used in robotics and AI to structure the logic of autonomous agents. It organizes tasks into tree structures where nodes represent actions or conditions.

**Belief State**: In probabilistic robotics, the probability distribution over all possible world states that represents the robot's knowledge about the environment.

**Bluetooth Low Energy (BLE)**: A wireless communication technology designed for short-range, low-power applications, often used in robotics for connecting to sensors and peripherals.

## C

**Cartesian Space**: The 3D space defined by X, Y, Z coordinates, used to describe positions and orientations of objects in the world.

**Computer Vision**: A field of AI focused on enabling computers to interpret and understand visual information from the world, crucial for robot perception.

**Control Theory**: The mathematical study of how to influence the behavior of dynamical systems, fundamental to robot movement and manipulation.

**Convolutional Neural Network (CNN)**: A class of deep neural networks commonly used in computer vision tasks, effective at recognizing patterns in images.

## D

**Deep Learning**: A subset of machine learning that uses neural networks with many layers (deep neural networks) to model complex patterns in data.

**Digital Twin**: A virtual replica of a physical system, used for simulation, testing, and optimization before deployment on the actual physical system.

**Differential Drive**: A common mobile robot configuration using two independently driven wheels on a common axis, allowing steering by varying the speeds of the wheels.

**Dynamic Movement Primitive (DMP)**: A movement representation approach in robotics that encodes and reproduces complex motor skills.

## E

**Embodied AI**: Artificial intelligence systems that interact with and learn from their physical environment through a body or robot platform.

**End Effector**: The device at the end of a robot arm that interacts with the environment, such as a gripper or welding tool.

**Euclidean Distance**: The straight-line distance between two points in Euclidean space, calculated using the Pythagorean theorem.

## F

**Forward Kinematics**: The process of determining the position and orientation of the end-effector based on the joint angles of a robot.

**Fiducial Marker**: A visual marker with a known geometric shape and pattern used in computer vision and robotics for position detection and camera pose estimation.

## G

**Gazebo**: A 3D simulation environment used for robotics, offering physics simulation, sensor simulation, and realistic virtual worlds.

**Generalized Coordinates**: A set of parameters that define the configuration of a mechanical system, such as joint angles in a robotic arm.

**Generative Adversarial Network (GAN)**: A class of machine learning frameworks where two neural networks contest with each other to generate new data.

**Geometry_msgs**: A ROS package defining common geometric primitive message types such as points, vectors, and poses.

## H

**Haptic Feedback**: The use of touch and motion in human-computer interaction, allowing robots to feel forces, textures, and vibrations.

**Heuristic**: A technique designed for solving problems more quickly when classic methods are too slow or for finding approximate solutions when classic methods fail to find exact solutions.

**Human-Robot Interaction (HRI)**: The field of study dedicated to understanding, designing, and evaluating robotic systems for human interaction.

## I

**Inverse Kinematics**: The process of determining the joint angles required to place the end-effector at a desired position and orientation.

**Isaac Sim**: NVIDIA's robotics simulator that enables development, testing, and training of AI-based robots in a physically accurate virtual environment.

**Image Segmentation**: The process of partitioning a digital image into multiple segments to simplify image analysis, crucial for robot perception.

## J

**Joint Space**: The space defined by the joint angles of a robot, as opposed to Cartesian space.

**JSON (JavaScript Object Notation)**: A lightweight data-interchange format used in robotics for configuration files and data transmission.

## K

**Kinematics**: The study of motion without considering the forces that cause the motion, fundamental in robot motion planning.

**Kubernetes**: An open-source container orchestration system, sometimes used in robotics for managing distributed robot systems and cloud-based processing.

## L

**Learning Rate**: A hyperparameter in machine learning that controls how much to change the model in response to the estimated error each time the model weights are updated.

**LiDAR (Light Detection and Ranging)**: A remote sensing method that uses light in the form of a pulsed laser to measure distances, commonly used in robotics for mapping and navigation.

**Long Short-Term Memory (LSTM)**: A type of recurrent neural network architecture that excels at modeling temporal sequences in robotics tasks.

## M

**Machine Learning**: A method of data analysis that automates analytical model building, enabling robots to improve their performance through experience.

**Manipulation**: The branch of robotics dealing with how robots interact physically with objects in their environment.

**Mapping**: The process of creating a representation of the environment for navigation and localization.

**Markov Decision Process (MDP)**: A mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision maker.

**Middleware**: Software that provides common services and capabilities to applications beyond what's offered by the operating system, such as ROS for robotics.

## N

**Navigation Stack**: A set of ROS packages that implement localization, path planning, and obstacle avoidance for mobile robots.

**Natural Language Processing (NLP)**: A field of AI focused on the interaction between computers and human language, enabling voice commands for robots.

**Neural Network**: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics the way the human brain operates.

**NVIDIA Isaac**: A collection of GPU-accelerated AI frameworks, libraries, and tools for robotics, enabling perception, navigation, and manipulation.

## O

**Occupancy Grid**: A probabilistic map representation where each cell stores the probability of occupancy, commonly used in mobile robot navigation.

**Odometry**: The use of data from motion sensors to estimate change in position over time, fundamental for robot localization.

**OpenAI**: An artificial intelligence research laboratory consisting of the for-profit OpenAI LP and the non-profit OpenAI Inc., creators of GPT and Whisper models used in robotics applications.

**Operational Space**: The space in which the robot's end-effector operates, typically described in Cartesian coordinates.

## P

**Path Planning**: The process of determining a route from a starting point to a destination in an environment, considering obstacles and navigation constraints.

**PID Controller**: A control loop mechanism employing feedback widely used in robotics for precise control of systems.

**Perception**: The process by which robots acquire and interpret sensory information from their environment.

**Point Cloud**: A set of data points in space, representing the external surface of an object or environment, commonly generated by LiDAR or depth sensors.

**Pose**: The position and orientation of an object in space, typically represented as 6 degrees of freedom (3D position and 3D rotation).

**Probabilistic Robotics**: An approach to robotics that explicitly deals with uncertainty in sensing and acting through the use of probability theory.

## Q

**Quaternion**: A mathematical concept used to represent rotations in 3D space, preferred in robotics over Euler angles to avoid gimbal lock.

## R

**Reinforcement Learning (RL)**: A type of machine learning where agents learn to make decisions by performing actions and receiving rewards or penalties, applicable to robotics behavior learning.

**Robot Operating System (ROS)**: Flexible framework for writing robot software that provides services designed for a heterogeneous computer cluster.

**ROS 2**: The second generation of ROS, featuring improvements in real-time support, security, and overall system architecture.

**Reactive System**: A system that responds to changes in its environment immediately, without internal deliberation, used in robot control systems.

**Reachability Analysis**: The process of determining what states can be reached from a given initial state, important in robot motion planning.

## S

**Sensor Fusion**: The process of combining data from multiple sensors to improve reliability and accuracy of environmental perception.

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location within it.

**State Estimation**: The process of estimating the internal state of a system from measurements, crucial for robot autonomy.

**Supervised Learning**: A machine learning paradigm where the algorithm learns from labeled training data, applicable to robot perception tasks.

**System Identification**: The process of determining mathematical models of dynamic systems from measured input-output data, used in robotics for control design.

## T

**Teleoperation**: The remote operation of a robot, typically used for training and safety in robotics development.

**Trajectory**: A time-parameterized path that specifies the desired position and orientation of a robot over time.

**Transformer Model**: A type of neural network architecture that uses attention mechanisms, fundamental to modern NLP and used in robotics for natural language understanding.

**Twist Message**: A ROS message type that represents the velocity of a rigid body in free space, with linear and angular components.

## V

**VLA (Vision-Language-Action)**: A model architecture that integrates vision, language understanding, and action execution for robotics tasks.

**Variational Autoencoder (VAE)**: A type of neural network used for unsupervised learning, applicable to representation learning in robotics.

**Velocity**: The rate of change of displacement of a robot, typically represented as linear and angular components.

**Visuomotor Control**: The control of robot movement based on visual input, essential for robot manipulation.

## W

**Waypoint**: A set of coordinates that defines a point in physical space used for navigation planning.

**Whisper**: OpenAI's automatic speech recognition (ASR) system used for converting voice commands to text in robotics applications.

**Workspace**: The volume of space that a robot can reach with its end-effector, important in manipulation planning.

## X, Y, Z

**XYZ**: Coordinate system axes (X-right, Y-forward, Z-up) commonly used in robotics for spatial representations.

**Yaw**: The rotation around the vertical (Z) axis, one of the three rotational degrees of freedom.

**Z-axis**: The vertical axis in the standard robotics coordinate system, typically pointing upward.